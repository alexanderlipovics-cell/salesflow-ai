# âœ… Implementation Checklist - Neue Features

## Was ist bereits fertig? âœ…

1. âœ… **Datenbank-Migration** erstellt
   - Datei: `supabase/migrations/20250115_commission_tracker_and_features.sql`
   - Tabellen: commissions, closing_insights, performance_insights, gamification, cold_call_sessions, route_plans

2. âœ… **Backend-Router** erstellt
   - `backend/app/routers/commissions.py` - Provisions-Tracker
   - `backend/app/routers/closing_coach.py` - Closing Coach
   - `backend/app/routers/cold_call_assistant.py` - Kaltakquise-Assistent

3. âœ… **LLM-Prompts** erstellt
   - `backend/app/prompts/closing_coach_prompts.py`
   - `backend/app/prompts/cold_call_prompts.py`
   - `backend/app/prompts/performance_coach_prompts.py`

4. âœ… **Router registriert** in `backend/app/main.py`

---

## Was musst du jetzt machen? ğŸ“‹

### Schritt 1: Datenbank-Migration ausfÃ¼hren âš ï¸ WICHTIG

```bash
# Migration ausfÃ¼hren (Supabase CLI)
supabase migration up

# ODER manuell in Supabase Dashboard:
# 1. Gehe zu Supabase Dashboard â†’ SQL Editor
# 2. Kopiere Inhalt von: supabase/migrations/20250115_commission_tracker_and_features.sql
# 3. FÃ¼hre aus
```

**PrÃ¼fen ob Migration erfolgreich:**
```sql
-- In Supabase SQL Editor ausfÃ¼hren
SELECT table_name 
FROM information_schema.tables 
WHERE table_schema = 'public' 
AND table_name IN ('commissions', 'closing_insights', 'cold_call_sessions');
```

---

### Schritt 2: LLM-Integration in Router einbauen ğŸ”§

Die Router haben aktuell **Placeholder-Logik**. Du musst die echten LLM-Calls einbauen.

#### Option A: Mit GPT (OpenAI)

1. **API Key setzen:**
```bash
# In .env oder Environment Variables
OPENAI_API_KEY=sk-...
```

2. **In `backend/app/routers/closing_coach.py` einbauen:**

```python
# Am Anfang der Datei hinzufÃ¼gen:
import openai
import json
from app.prompts.closing_coach_prompts import get_closing_coach_gpt_prompt
from app.config import get_settings

settings = get_settings()

# Ersetze die Funktion analyze_deal_for_closing():
async def analyze_deal_for_closing(deal_data: dict, conversation_history: List[dict]) -> dict:
    """Analysiert Deal mit GPT."""
    try:
        prompt = get_closing_coach_gpt_prompt(deal_data, conversation_history)
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4",
            messages=prompt,
            temperature=0.3,
            max_tokens=2000
        )
        
        result_text = response.choices[0].message.content
        result = json.loads(result_text)
        return result
        
    except Exception as e:
        # Fallback auf Placeholder
        return {
            "detected_blockers": [],
            "closing_score": 50.0,
            "closing_probability": "medium",
            "recommended_strategies": [],
            "suggested_next_action": "Follow-up planen",
            "objection_count": 0,
            "price_mentioned_count": 0,
        }
```

#### Option B: Mit Claude (Anthropic)

```python
import anthropic
import json
from app.prompts.closing_coach_prompts import get_closing_coach_claude_prompt

async def analyze_deal_for_closing(deal_data: dict, conversation_history: List[dict]) -> dict:
    """Analysiert Deal mit Claude."""
    try:
        prompt = get_closing_coach_claude_prompt(deal_data, conversation_history)
        
        client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        response = await client.messages.create(
            model="claude-3-opus-20240229",
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        result_text = response.content[0].text
        result = json.loads(result_text)
        return result
        
    except Exception as e:
        # Fallback
        ...
```

#### Option C: Mit Gemini (Google)

```python
import google.generativeai as genai
import json
from app.prompts.closing_coach_prompts import get_closing_coach_gemini_prompt

async def analyze_deal_for_closing(deal_data: dict, conversation_history: List[dict]) -> dict:
    """Analysiert Deal mit Gemini."""
    try:
        genai.configure(api_key=settings.GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-pro')
        
        prompt = get_closing_coach_gemini_prompt(deal_data, conversation_history)
        response = await model.generate_content_async(prompt)
        
        result = json.loads(response.text)
        return result
        
    except Exception as e:
        # Fallback
        ...
```

**WICHTIG:** Mache das gleiche fÃ¼r:
- `cold_call_assistant.py` â†’ `generate_personalized_script()`
- `performance_coach.py` (noch zu erstellen) â†’ Performance-Analyse

---

### Schritt 3: Frontend-Komponenten erstellen ğŸ¨

Erstelle React-Komponenten fÃ¼r die neuen Features:

#### 3.1 Provisions-Tracker Page

**Datei:** `src/pages/CommissionTrackerPage.tsx`

```typescript
import { useState, useEffect } from 'react';
import { useNavigate } from 'react-router-dom';

export default function CommissionTrackerPage() {
  const [commissions, setCommissions] = useState([]);
  const [selectedMonth, setSelectedMonth] = useState(new Date());

  // API-Call: GET /api/commissions?month=2025-01-01
  useEffect(() => {
    fetchCommissions();
  }, [selectedMonth]);

  return (
    <div>
      <h1>Meine Provisionen</h1>
      {/* Monatsauswahl */}
      {/* Liste der Provisionen */}
      {/* PDF-Export Button */}
      {/* "An Buchhaltung" Button */}
    </div>
  );
}
```

#### 3.2 Closing Coach Page

**Datei:** `src/pages/ClosingCoachPage.tsx`

```typescript
export default function ClosingCoachPage() {
  // API-Call: GET /api/closing-coach/my-deals
  // Zeige Deals mit Closing-Score
  // Blocker-Anzeige
  // Empfohlene Strategien
}
```

#### 3.3 Cold Call Assistant Page

**Datei:** `src/pages/ColdCallAssistantPage.tsx`

```typescript
export default function ColdCallAssistantPage() {
  // API-Call: POST /api/cold-call/generate-script/{contact_id}
  // Zeige Script
  // Session-Manager
  // Ãœbungsmodus
}
```

**Routing hinzufÃ¼gen in `src/App.jsx`:**
```jsx
<Route path="commissions" element={<CommissionTrackerPage />} />
<Route path="closing-coach" element={<ClosingCoachPage />} />
<Route path="cold-call" element={<ColdCallAssistantPage />} />
```

---

### Schritt 4: API-Keys konfigurieren ğŸ”‘

**Datei:** `backend/app/config.py` oder `.env`

```python
# .env
OPENAI_API_KEY=sk-...
# ODER
ANTHROPIC_API_KEY=sk-ant-...
# ODER
GEMINI_API_KEY=...
```

**In `backend/app/config.py` hinzufÃ¼gen:**
```python
class Settings(BaseSettings):
    # ... bestehende Settings
    openai_api_key: Optional[str] = None
    anthropic_api_key: Optional[str] = None
    gemini_api_key: Optional[str] = None
```

---

### Schritt 5: Testen ğŸ§ª

1. **Backend testen:**
```bash
cd backend
python -m pytest tests/test_commissions.py
python -m pytest tests/test_closing_coach.py
```

2. **API-Endpunkte testen:**
```bash
# Mit curl oder Postman
curl -X GET http://localhost:8000/api/commissions \
  -H "Authorization: Bearer YOUR_TOKEN"
```

3. **Frontend testen:**
- Ã–ffne `/commissions` im Browser
- Ã–ffne `/closing-coach` im Browser
- Ã–ffne `/cold-call` im Browser

---

## Priorisierung ğŸ¯

**Wenn du wenig Zeit hast, mache zuerst:**

1. âœ… Migration ausfÃ¼hren (5 Min)
2. âœ… Einen LLM-Provider wÃ¤hlen (GPT/Claude/Gemini) (10 Min)
3. âœ… LLM-Integration in **einen** Router (Closing Coach) (30 Min)
4. âœ… Eine Frontend-Page (Closing Coach) (1-2h)

**Dann spÃ¤ter:**
- Restliche Router
- Restliche Frontend-Pages
- Gamification
- Route Planner

---

## Hilfe benÃ¶tigt? ğŸ’¬

- **Migration-Probleme?** â†’ PrÃ¼fe Supabase Logs
- **LLM-Integration?** â†’ Siehe `docs/LLM_PROMPTS_GUIDE.md`
- **Frontend?** â†’ Nutze bestehende Pages als Vorlage (z.B. `ChatPage.tsx`)

---

## Zusammenfassung ğŸ“

**Du musst JETZT machen:**
1. âš ï¸ Migration ausfÃ¼hren (wichtig!)
2. ğŸ”§ LLM-Integration in Router (wÃ¤hle einen Provider)
3. ğŸ¨ Frontend-Komponenten erstellen (oder spÃ¤ter)

**Alles andere kann warten!**

