"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  MENTOR AI SERVICE                                                         ‚ïë
‚ïë  Hauptservice f√ºr MENTOR AI Chat                                           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""

import time
import uuid
import logging
from typing import Optional, List, Dict, Any
from dataclasses import dataclass
from datetime import datetime
from supabase import Client

from ..llm_client import LLMClient, get_llm_client
from .prompts import MENTOR_SYSTEM_PROMPT, MENTOR_CONTEXT_TEMPLATE, DISC_ADAPTATION_PROMPT
from .action_parser import ActionParser, ActionTag, extract_action_tags, strip_action_tags
from .context_builder import MentorContextBuilder, MentorContext
from ...core.config import settings
from ...services.compliance import check_message, ComplianceResult, ComplianceSentinel
from ...config.knowledge.mentor_knowledge import (
    build_mentor_system_prompt_addition,
    get_company_context,
)
from ...services.mentor.chief_service import is_chief_user, check_chief_access

# Neue Prompt-Struktur
import sys
from pathlib import Path
backend_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(backend_root))
from prompts import (
    get_full_prompt,
    get_network_marketing_prompt,
    get_field_sales_prompt,
    get_general_prompt,
    get_phoenix_prompt,
    get_delay_master_prompt,
    get_dmo_tracker_prompt,
    get_ghostbuster_prompt,
)

logger = logging.getLogger(__name__)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# RESPONSE TYPES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class MentorResponse:
    """Response vom MENTOR AI Service."""
    response: str
    actions: List[ActionTag]
    tokens_used: Optional[int] = None
    latency_ms: Optional[int] = None
    context_summary: Optional[Dict[str, Any]] = None
    interaction_id: Optional[str] = None
    compliance_warning: Optional[Dict[str, Any]] = None  # Compliance-Warnung falls vorhanden
    
    def to_dict(self) -> dict:
        """Konvertiert zu Dictionary f√ºr API Response."""
        return {
            "response": self.response,
            "actions": [a.to_dict() for a in self.actions],
            "tokens_used": self.tokens_used,
            "context_summary": self.context_summary,
            "compliance_warning": self.compliance_warning,
        }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MENTOR SERVICE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class MentorService:
    """
    MENTOR AI Service.
    
    Hauptservice f√ºr Chat mit dem AI Sales Coach.
    
    Usage:
        service = MentorService(db)
        response = await service.chat(
            user_id="...",
            message="Wie steh ich heute?",
            include_context=True
        )
    """
    
    def __init__(
        self,
        db: Client,
        llm_client: Optional[LLMClient] = None,
    ):
        self.db = db
        self.llm = llm_client or get_llm_client()
        self.action_parser = ActionParser()
        self.context_builder = MentorContextBuilder(db)
    
    async def chat(
        self,
        user_id: str,
        message: str,
        company_id: Optional[str] = None,
        user_name: Optional[str] = None,
        include_context: bool = True,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        disc_type: Optional[str] = None,
        model: Optional[str] = None,
        temperature: float = 0.7,
    ) -> MentorResponse:
        """
        F√ºhrt einen Chat mit MENTOR durch.
        
        Args:
            user_id: User ID
            message: Die User-Nachricht
            company_id: Company ID (optional)
            user_name: User Name (override)
            include_context: Kontext einbeziehen?
            conversation_history: Bisheriger Chatverlauf
            disc_type: DISC-Typ f√ºr Lead-Anpassung (D, I, S, G)
            model: LLM Model (override)
            temperature: LLM Temperature
            
        Returns:
            MentorResponse mit AI Antwort und Actions
        """
        start_time = time.time()
        interaction_id = str(uuid.uuid4())
        
        # CHIEF Mode Check
        is_chief = False
        try:
            # Versuche User Email zu holen
            user_result = self.db.table("profiles").select("email").eq("id", user_id).single().execute()
            if user_result.data:
                user_email = user_result.data.get("email")
                if user_email:
                    is_chief = is_chief_user(user_email)
        except Exception as e:
            logger.warning(f"Could not check CHIEF access: {e}")
            # Fallback: Pr√ºfe √ºber check_chief_access
            is_chief = await check_chief_access(self.db, user_id)
        
        # Context aufbauen
        context = None
        context_text = None
        if include_context:
            try:
                context = await self.context_builder.build(
                    user_id=user_id,
                    company_id=company_id,
                    user_name=user_name,
                )
                context_text = context.to_text()
            except Exception as e:
                logger.warning(f"Could not build context: {e}")
        
        # Vertical und Module aus Context holen
        vertical = None
        skill_level = None
        enabled_modules = None
        if context:
            vertical = context.vertical
            skill_level = getattr(context, 'skill_level', None) or context.experience_level
            enabled_modules = getattr(context, 'enabled_modules', None)
        
        # Messages zusammenbauen
        messages = self._build_messages(
            message=message,
            context_text=context_text,
            conversation_history=conversation_history,
            disc_type=disc_type,
            vertical=vertical,
            skill_level=skill_level,
            enabled_modules=enabled_modules,
            company_id=company_id,
            is_chief=is_chief,
        )
        
        # LLM aufrufen
        try:
            raw_response = await self.llm.chat(
                messages=messages,
                model=model,
                temperature=temperature,
            )
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            raise
        
        # Response parsen
        parse_result = self.action_parser.parse(raw_response)
        
        # Compliance-Check f√ºr User-Nachricht (Input)
        input_compliance_result = None
        company_name = None
        if company_id:
            # Konvertiere company_id zu company_name f√ºr Compliance Sentinel
            company_name = company_id.lower()
        
        # Pr√ºfe User-Nachricht auf Compliance
        if company_name:
            input_compliance_result = check_message(message, company=company_name)
        
        # Compliance-Check f√ºr generierte Nachricht (Response)
        compliance_warning = None
        response_compliance_result = check_message(parse_result.clean_text, company=company_name)
        
        # Wenn Verst√∂√üe in Response gefunden wurden, f√ºge Warning hinzu
        if not response_compliance_result.is_compliant and response_compliance_result.violations:
            # Erstelle Warning-Objekt
            critical_violations = [v for v in response_compliance_result.violations if v.severity == "critical"]
            error_violations = [v for v in response_compliance_result.violations if v.severity == "error"]
            
            if critical_violations or error_violations:
                compliance_warning = {
                    "has_violations": True,
                    "risk_score": response_compliance_result.risk_score,
                    "violation_count": len(response_compliance_result.violations),
                    "critical_count": len(critical_violations),
                    "error_count": len(error_violations),
                    "violations": [
                        {
                            "type": v.type.value,
                            "severity": v.severity,
                            "matched_text": v.matched_text,
                            "explanation": v.explanation,
                            "suggestion": v.suggestion,
                        }
                        for v in response_compliance_result.violations[:5]  # Max 5 f√ºr Response
                    ],
                    "message": (
                        f"‚ö†Ô∏è Compliance-Warnung: {len(response_compliance_result.violations)} Versto√ü(e) in der Antwort gefunden. "
                        f"Bitte √ºberpr√ºfe die Nachricht vor dem Versenden."
                    ),
                }
                logger.warning(
                    f"Compliance-Versto√ü in MENTOR Response f√ºr User {user_id}: "
                    f"{len(response_compliance_result.violations)} Verst√∂√üe, Risk Score: {response_compliance_result.risk_score}"
                )
        
        # Warnung auch bei Input-Verst√∂√üen (wenn kritisch)
        if input_compliance_result and not input_compliance_result.is_compliant:
            critical_input = [v for v in input_compliance_result.violations if v.severity == "critical"]
            if critical_input and not compliance_warning:
                compliance_warning = {
                    "has_violations": True,
                    "risk_score": input_compliance_result.risk_score,
                    "violation_count": len(input_compliance_result.violations),
                    "critical_count": len(critical_input),
                    "message": (
                        f"‚ö†Ô∏è Deine Nachricht enth√§lt m√∂gliche Compliance-Verst√∂√üe. "
                        f"Bitte formuliere deine Frage um."
                    ),
                }
        
        # Latenz berechnen
        latency_ms = int((time.time() - start_time) * 1000)
        
        # Tokens sch√§tzen
        tokens_estimate = (len(str(messages)) + len(raw_response)) // 4
        
        # Context Summary
        context_summary = None
        if context:
            context_summary = {
                "daily_flow_completion": context.daily_flow.get("overall_percent", 0) if context.daily_flow else None,
                "suggested_leads_count": len(context.suggested_leads) if context.suggested_leads else 0,
                "streak_days": context.streak_days,
            }
        
        # AI Interaction loggen
        await self._log_interaction(
            interaction_id=interaction_id,
            user_id=user_id,
            company_id=company_id,
            message=message,
            response=parse_result.clean_text,
            actions=parse_result.actions,
            tokens=tokens_estimate,
            latency_ms=latency_ms,
        )
        
        return MentorResponse(
            response=parse_result.clean_text,
            actions=parse_result.actions,
            tokens_used=tokens_estimate,
            latency_ms=latency_ms,
            context_summary=context_summary,
            interaction_id=interaction_id,
            compliance_warning=compliance_warning,
        )
    
    def _build_messages(
        self,
        message: str,
        context_text: Optional[str],
        conversation_history: Optional[List[Dict[str, str]]],
        disc_type: Optional[str],
        vertical: Optional[str] = None,
        skill_level: Optional[str] = None,
        enabled_modules: Optional[List[str]] = None,
        company_id: Optional[str] = None,
        is_chief: bool = False,
    ) -> List[Dict[str, str]]:
        """Baut die Messages f√ºr den LLM Call."""
        messages = []
        
        # MENTOR System Prompt mit Knowledge
        system_prompt = self._build_mentor_system_prompt(
            company_id=company_id,
            is_chief=is_chief,
        )
        
        # Neue Prompt-Struktur verwenden wenn Vertical bekannt
        if vertical:
            # Vertical-spezifischer Prompt laden
            vertical_prompt = None
            if vertical == "network_marketing":
                vertical_prompt = get_network_marketing_prompt()
            elif vertical == "field_sales":
                vertical_prompt = get_field_sales_prompt()
            else:
                vertical_prompt = get_general_prompt()
            
            # Module Prompts zusammenbauen
            module_prompts = []
            if enabled_modules:
                if "phoenix" in enabled_modules:
                    module_prompts.append(get_phoenix_prompt())
                if "delay_master" in enabled_modules:
                    module_prompts.append(get_delay_master_prompt())
                if "dmo_tracker" in enabled_modules:
                    module_prompts.append(get_dmo_tracker_prompt())
                if "ghostbuster" in enabled_modules:
                    module_prompts.append(get_ghostbuster_prompt())
            
            # Vollst√§ndigen Prompt bauen
            messages = get_full_prompt(
                vertical=vertical or "network_marketing",
                skill_level=skill_level or "advanced",
                vertical_specific_prompt=vertical_prompt,
                enabled_modules=enabled_modules or [],
                context_text=context_text,
            )
            
            # MENTOR System Prompt am Anfang einf√ºgen
            if messages and messages[0].get("role") == "system":
                # Erweitere ersten System Prompt mit MENTOR Knowledge
                messages[0]["content"] = system_prompt + "\n\n" + messages[0]["content"]
            else:
                messages.insert(0, {
                    "role": "system",
                    "content": system_prompt,
                })
        else:
            # Fallback: Alte Prompts mit MENTOR Knowledge
            messages.append({
                "role": "system",
                "content": system_prompt,
            })
            
            # Context hinzuf√ºgen
            if context_text:
                messages.append({
                    "role": "system",
                    "content": MENTOR_CONTEXT_TEMPLATE.format(context_text=context_text),
                })
        
        # DISC Adaptation
        if disc_type and disc_type in ["D", "I", "S", "G"]:
            messages.append({
                "role": "system",
                "content": DISC_ADAPTATION_PROMPT.format(disc_type=disc_type),
            })
        
        # Conversation History
        if conversation_history:
            max_history = getattr(settings, "MAX_CONVERSATION_HISTORY", 10)
            for msg in conversation_history[-max_history:]:
                if msg.get("role") in ["user", "assistant"]:
                    messages.append({
                        "role": msg["role"],
                        "content": msg["content"],
                    })
        
        # Aktuelle User Message
        messages.append({
            "role": "user",
            "content": message,
        })
        
        return messages
    
    def _build_mentor_system_prompt(
        self,
        company_id: Optional[str] = None,
        is_chief: bool = False,
    ) -> str:
        """
        Baut den MENTOR System Prompt mit Knowledge Integration.
        
        Args:
            company_id: Company ID f√ºr company-spezifische Knowledge
            is_chief: Ist der User ein CHIEF User (Founder)?
            
        Returns:
            Vollst√§ndiger System Prompt
        """
        # Basis System Prompt
        base_prompt = """Du bist MENTOR, der KI-Vertriebscoach f√ºr Network Marketing.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
DEINE EXPERTISE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚Ä¢ Follow-up Strategien
‚Ä¢ Einwandbehandlung (zu teuer, keine Zeit, muss √ºberlegen)
‚Ä¢ HWG/DSGVO-konforme Kommunikation
‚Ä¢ Gespr√§chseinstiege und Abschluss-Techniken

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
DEIN STIL
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚Ä¢ Freundlich aber direkt
‚Ä¢ Praxisorientiert mit konkreten Formulierungen
‚Ä¢ Kurze, actionable Antworten
‚Ä¢ Deutsch

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
COMPLIANCE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Bei Compliance-Fragen: Pr√ºfe auf HWG (Heilversprechen) und DSGVO.
NIEMALS Heilversprechen machen oder Krankheitsnamen nennen.
NUR: Wellness, Lifestyle, Wohlbefinden, kosmetische Claims.
"""
        
        # CHIEF Mode - Erweiterte Prompts und alle Skripte
        if is_chief:
            chief_addition = """
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üëë CHIEF MODE AKTIVIERT
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Du hast Zugang zu ALLEN Power-Features:

‚úÖ 50+ Outreach Skripte (Zinzino, B2B, Immobilien, Hotels)
‚úÖ Erweiterte Einwandbehandlung (SalesFlow-spezifisch)
‚úÖ Deal-Medic Prompts (BANT-Analyse, Pipeline-Review)
‚úÖ CEO Module (Strategische Frameworks)
‚úÖ Investor Brief Generation
‚úÖ Keine API Limits

Nutze alle verf√ºgbaren Ressourcen f√ºr beste Ergebnisse!
"""
            base_prompt += "\n\n" + chief_addition
        
        # Company-spezifische Knowledge hinzuf√ºgen
        if company_id:
            try:
                company_name = company_id.lower()
                knowledge_addition = build_mentor_system_prompt_addition(company_name)
                if knowledge_addition:
                    base_prompt += "\n\n" + knowledge_addition
            except Exception as e:
                logger.warning(f"Could not load company knowledge for {company_id}: {e}")
        
        return base_prompt
    
    async def _log_interaction(
        self,
        interaction_id: str,
        user_id: str,
        company_id: Optional[str],
        message: str,
        response: str,
        actions: List[ActionTag],
        tokens: int,
        latency_ms: int,
    ):
        """Loggt die AI Interaction in die Datenbank."""
        try:
            self.db.table("ai_interactions").insert({
                "id": interaction_id,
                "user_id": user_id,
                "company_id": company_id,
                "interaction_type": "mentor_chat",
                "request_text": message[:2000],  # Limit
                "response_text": response[:4000],  # Limit
                "model": self.llm.model,
                "provider": self.llm.provider,
                "total_tokens": tokens,
                "latency_ms": latency_ms,
                "action_tags": [a.to_dict() for a in actions],
                "created_at": datetime.utcnow().isoformat(),
            }).execute()
        except Exception as e:
            # Logging should not fail the request
            logger.warning(f"Could not log AI interaction: {e}")
    
    async def get_context(
        self,
        user_id: str,
        company_id: Optional[str] = None,
    ) -> MentorContext:
        """
        Gibt den aktuellen Kontext zur√ºck (f√ºr Frontend).
        
        Args:
            user_id: User ID
            company_id: Company ID (optional)
            
        Returns:
            MentorContext Objekt
        """
        return await self.context_builder.build(
            user_id=user_id,
            company_id=company_id,
        )
    
    async def submit_feedback(
        self,
        interaction_id: str,
        user_id: str,
        rating: Optional[int] = None,
        feedback: Optional[str] = None,
        was_helpful: Optional[bool] = None,
    ) -> bool:
        """
        Speichert Feedback zu einer AI Interaction.
        
        Args:
            interaction_id: ID der Interaction
            user_id: User ID (f√ºr Validierung)
            rating: Bewertung (1-5)
            feedback: Text-Feedback
            was_helpful: War die Antwort hilfreich?
            
        Returns:
            True wenn erfolgreich
        """
        try:
            update_data = {}
            if rating is not None:
                update_data["user_rating"] = rating
            if feedback is not None:
                update_data["user_feedback"] = feedback
            if was_helpful is not None:
                update_data["was_helpful"] = was_helpful
            
            if update_data:
                self.db.table("ai_interactions").update(update_data).eq(
                    "id", interaction_id
                ).eq("user_id", user_id).execute()
            
            return True
        except Exception as e:
            logger.error(f"Could not save feedback: {e}")
            return False


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FACTORY
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_mentor_service(db: Client) -> MentorService:
    """
    Factory Function f√ºr MentorService.
    
    Args:
        db: Supabase Client
        
    Returns:
        MentorService Instance
    """
    return MentorService(db)

